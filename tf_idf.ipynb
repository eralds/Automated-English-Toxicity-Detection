{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e47eb7e-845f-4a6d-858b-d6da7cf948ca",
   "metadata": {},
   "source": [
    "# TF IDF embedding\n",
    "\n",
    "This notebook does the preprocessing and cleaning of text files through the Cleaner class\n",
    "\n",
    "The TfIdfEmbedder class calculates the tfidf embeddings for a given matrix\n",
    "\n",
    "The commented parts explain extra functionality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77311295-f29a-43bc-8148-156469f9397b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "np.int = np.int32\n",
    "np.float = np.float64\n",
    "np.bool = np.bool_\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# extra\n",
    "# import time\n",
    "import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "#to save and load objects\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f52837-aad0-4f5b-a856-84aa2961442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we use PyTorch later\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc05510-c59d-4dea-a81d-b569c7295fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Needed for stopwords, and lemmatization\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_english = stopwords.words('english')\n",
    "\n",
    "# !pip install stanza\n",
    "import stanza\n",
    "stanza.download('en', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d63280e-d5ea-479d-af09-6a93d74cfc1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loading training and testing data\n",
    "train_df = pd.read_csv(\"train_2024.csv\", quoting=csv.QUOTE_NONE)\n",
    "val_df = pd.read_csv(\"dev_2024.csv\", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "Text_train = train_df[\"text\"].values\n",
    "y_train = train_df[\"label\"].values\n",
    "\n",
    "Text_val = val_df[\"text\"].values\n",
    "y_val = val_df[\"label\"].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f81b5f-cc13-4e1c-a513-179ada2c14e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cleaner class\n",
    "\n",
    "Preprocesses the training data stored in the cleaned atribute. Tokenizes and lemmatizes\n",
    "\n",
    "Works also on the query data\n",
    "\n",
    "The training cleaned matrix can be saved an loaded\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class Cleaner():\n",
    "    def __init__(self,use_gpu=False):\n",
    "        self.parser = stanza.Pipeline(lang='en', processors='tokenize, lemma',  verbose=False, use_gpu=use_gpu)\n",
    "        self.cleaned = []\n",
    "\n",
    "    #save and load the cleaned matrix\n",
    "    def save_cleaned_matrix(self, name):\n",
    "        with open(name, 'wb') as f:\n",
    "            np.save(f, np.array(self.cleaned))\n",
    "            \n",
    "    def load_cleaned_matrix(self, name):\n",
    "        with open(name, 'rb') as f:\n",
    "            self.cleaned = np.load(f)\n",
    "\n",
    "    def clean(self,text):\n",
    "        self.cleaned.append(' '.join([word.lemma for sentence in self.parser(text).iter_tokens() for word in sentence.words]))\n",
    "\n",
    "    def query_clean(self,texts):\n",
    "        cleaned_texts = []\n",
    "        for text in texts:\n",
    "            cleaned_texts.append(' '.join([word.lemma for sentence in self.parser(text).iter_tokens() for word in sentence.words])) \n",
    "        return cleaned_texts\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c2d1e0-65c9-4cc3-86df-d4ed458e1629",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tf Idf class\n",
    "\n",
    "Vecorizes the given cleaned training matrix and stores the tfidf matrix\n",
    "\n",
    "The tfidf object does the transformation of the query clened vector\n",
    "\n",
    "\"\"\"\n",
    "class TfIdfEmbedder():\n",
    "    def __init__(self,max_features=None,stop_words=None ):\n",
    "        self.max_features = max_features\n",
    "        self.stopwords = stop_words\n",
    "        self.tfidf = TfidfVectorizer(stop_words=self.stopwords,max_features=self.max_features)\n",
    "        self.builded = False\n",
    "        self.tfidf_matrix = None\n",
    "        \n",
    "        \n",
    "    def build_vectorizer(self, cleaned_text):\n",
    "        self.builded = True\n",
    "        self.tfidf_matrix = self.tfidf.fit_transform(cleaned_text)\n",
    "        return self.tfidf_matrix \n",
    "        \n",
    "    def vectorize_query(self,cleaned_texts):\n",
    "        if self.builded:\n",
    "            return self.tfidf.transform(cleaned_texts)\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Vectorizer is not builded.\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35256a4e-0a67-4268-9eae-d394d0d5f26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# #Clean the training and testing data and save the cleaned versions for later\n",
    "Cl = Cleaner(  use_gpu = True)\n",
    "for i,text in enumerate(Text_train):\n",
    "    if i%1000 == 0:\n",
    "        print(i)\n",
    "    Cl.clean(text)\n",
    "print(\"done\")\n",
    "\n",
    "TfIdf = TfIdfEmbedder(10000,stop_words_english)\n",
    "tf_idf_matrix = TfIdf.build_vectorizer(Cl.cleaned)\n",
    "cleaned_query = Cl.query_clean(Text_val)\n",
    "q_matrix = TfIdf.vectorize_query(cleaned_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "226e3d00-666f-45cf-8bf3-ce21480db2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/27/shahine1/unix/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8042813455657493\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "lr = svm.SVC(max_iter=10000)\n",
    "\n",
    "\n",
    "lr.fit(tf_idf_matrix, y_train)\n",
    "ypred = lr.predict(q_matrix)\n",
    "print(f1_score(ypred, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c27e26-c443-40ab-845f-272db0dfd479",
   "metadata": {},
   "source": [
    "Demostration of saving cleaned matrix of text to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7a3da0a-deb6-4cf6-80bf-23bc548d1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cl.save_cleaned_matrix(\"data/cleaned_train.npy\")\n",
    "# with open(\"data/cleaned_val.npy\", 'wb') as f:\n",
    "#     np.save(f, np.array(cleaned_query))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7959a6b7-0b1f-4ec3-b975-c064294c0981",
   "metadata": {},
   "source": [
    "Demostration of loading cleaned matrix of text to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5389708-2168-4f45-9bc4-241e45bf9bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# #create new classes\n",
    "# TfIdf2 = TfIdfEmbedder(10000, stop_words_english)\n",
    "# Cl2 = Cleaner()\n",
    "\n",
    "\n",
    "# #load\n",
    "# Cl2.load_cleaned_matrix(\"data/cleaned_train.npy\")\n",
    "# with open(\"data/cleaned_val.npy\", 'rb') as f:\n",
    "#     q_cleaned2 = np.load(f)\n",
    "\n",
    "# #get tfidf matrixes from cleaneddata\n",
    "# tf_idf_matrix2 = TfIdf2.build_vectorizer(Cl2.cleaned)\n",
    "# q_matrix2 = TfIdf2.vectorize_query(q_cleaned2)\n",
    "\n",
    "# #train new classifier\n",
    "# lr2 =  RandomForestClassifier()\n",
    "# lr2.fit(tf_idf_matrix2, y_train)\n",
    "\n",
    "# #get new score\n",
    "# ypred2 = lr2.predict(q_matrix2)\n",
    "# print(f1_score(ypred2, y_val))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911fa51-f20c-49ec-b483-6e16f179dce5",
   "metadata": {},
   "source": [
    "PCA Analysis (didn't produce good results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "265c3528-974e-4526-a913-ad8d5205bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import umap\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# n_topics = 50\n",
    "\n",
    "# # making latent topics\n",
    "# tf_idf_svd=TruncatedSVD(n_components=n_topics)\n",
    "# tf_idf_matrix_dense = tf_idf_svd.fit_transform(tf_idf_matrix2)\n",
    "# print(tf_idf_matrix_dense.shape)\n",
    "# # 2d transformation for visualization\n",
    "# tf_idf_umap = umap.UMAP(n_neighbors=10, n_components=2)\n",
    "# tf_idf_matrix_umap = tf_idf_umap.fit_transform(tf_idf_matrix_dense)\n",
    "# print(tf_idf_matrix_umap.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02d0daf4-2c02-44fa-839c-27a18cbb63cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_songs(song_matrix, title):\n",
    "#     \"\"\" Plots 2d vectors of songs and marks song with an artist label\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     song_matrix : numpy array\n",
    "#         columns are songs\n",
    "#         rows a latent topics\n",
    "#     title : str\n",
    "#         title for the plot\n",
    "#     songs_index : dict\n",
    "#         dictionary of artist and their songs indices in the song_matrix\n",
    "#     indices_to_remove : list\n",
    "#         list of songs to not plot\n",
    "#     \"\"\"\n",
    "\n",
    "#     plt.style.use('ggplot')\n",
    "#     # pulp_indices = songs_index['pulp']\n",
    "#     # princess_nokia_indices = songs_index['princess_nokia']\n",
    "#     # at_the_drive_in_indices = songs_index['at_the_drive_in']\n",
    "\n",
    "#     toxic_i = np.where(y_train == 1)[0]\n",
    "#     non_toxic_i = np.where(y_train == 0)[0]\n",
    "    \n",
    "#     plt.title(title)\n",
    "#     plt.xlabel(\"Feature 1\")\n",
    "#     plt.ylabel(\"Feature 2\")\n",
    "\n",
    "#     toxic = plt.scatter(song_matrix[toxic_i,0], song_matrix[toxic_i,1], marker=\"x\", color=\"red\")\n",
    "#     non_toxic = plt.scatter(song_matrix[non_toxic_i,0], song_matrix[non_toxic_i,1], marker=\"o\", color=\"cyan\")\n",
    "\n",
    "#     # at_the_drive_in = plt.scatter(song_matrix[0,at_the_drive_in_indices], song_matrix[1,at_the_drive_in_indices], marker=\"^\", color=\"black\")\n",
    "    \n",
    "#     plt.legend((toxic, non_toxic),('Toxic', 'Non-Toxic'))\n",
    "    \n",
    "#     plt.show()\n",
    "    \n",
    "# plot_songs(tf_idf_matrix_umap, \"Songs as 2-D vectors (tf-idf)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae14508-a64d-4d53-8139-5aa12dc1be6d",
   "metadata": {},
   "source": [
    "Saving the embedding object directly using the pickle library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96f9d8ae-3775-4024-89db-7684b15aab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the TfIdf object\n",
    "# with open('tfidf_object.pkl', 'wb') as f:\n",
    "#     pickle.dump(TfIdf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1f8d19b-74d3-4730-8f95-65e6ba6a1e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tfidf_object.pkl', 'rb') as f:\n",
    "#     TfIdf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd99970e-610d-4cde-a572-f98a5e9b0981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/cleaned_val.npy\", 'rb') as f:\n",
    "#     q_cleaned = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1f934-f889-44d1-9191-ab83654442b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8774437-8132-4af7-ab8e-97ccaaf9a369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:3457: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "#loading test data\n",
    "\n",
    "test_df = pd.read_csv(\"test_2024.csv\", quoting=csv.QUOTE_NONE, error_bad_lines=False)\n",
    "Text_test = test_df[\"text\"].to_numpy()\n",
    "\n",
    "y_matrix = TfIdf.vectorize_query(Cl.query_clean(Text_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c4447cb-768e-4851-aef9-bffd40bcdd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lr.predict(y_matrix)\n",
    "\n",
    "with open(\"testidf.csv\", \"w\") as f:\n",
    "    f.write(\"id,label\\n\")\n",
    "    for i,l in enumerate(y):\n",
    "        f.write(str(i)+\",\"+str(l) +\"\\n\")\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
